# -*- coding: utf-8 -*-
"""metaparameter-tuning-solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j4P5yC87NFxjNezbBgaLv1XnDR0EZLyi

# Metaparameter Tuning: Controlling LLM's Behaviour

### Below you're given a few tasks, tweak the relevant metaparameters in such a way that it enables the model to generate desired output.

Import the transformers library and relevant modules to run the model locally
"""

from transformers import pipeline
import warnings
warnings.filterwarnings('ignore')

generator = pipeline('text-generation', model='meta-llama/Llama-3.2-1B-Instruct')

"""**Task 1**

Below we are using llama to generate a story, but as you can see the output is getting cut-off in between before the story is complete. Tune the metaparameters in such a way that the model is able to generate the full story.
"""

response = generator("Write a 500 word story about a boy who gained superpowers by being bitten by a radioactive spider.",
                     max_length=1024,
                     num_return_sequences=1)
print(response)

"""**Task 2**

Awesome, now that we're able to generate the whole story, change the relevant metaparameter to make the story more creative.
"""

response = generator("Write a 500 word story about a boy who gained superpowers by being bitten by a radioactive spider.",
                     max_length=1024,
                     temperature = 0.9,
                     num_return_sequences=1)
print(response)

"""**Task 3**

Refer to the lectures and explore what other options (or metaparameters) we can use to control the behaviour of LLMs.
"""

response = generator("Write a 500 word story about a boy who gained superpowers by being bitten by a radioactive spider.",
                     max_length=1024,
                     temperature = 0.9,
                     top_k = 50,
                     top_p = 0.95,
                     num_return_sequences=1)
print(response)